"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                    â•‘
â•‘     API REST - DETECCIÃ“N DE NEUMONÃA CON IA                       â•‘
â•‘     Sistema de predicciÃ³n mediante Deep Learning                   â•‘
â•‘                                                                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECCIÃ“N 1: IMPORTACIONES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import tensorflow as tf
from tensorflow import keras
import numpy as np
from PIL import Image
import io
import base64
from datetime import datetime
import logging
from pathlib import Path

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECCIÃ“N 2: INICIALIZACIÃ“N DE FASTAPI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(
    title="API de DetecciÃ³n de NeumonÃ­a",
    description="API REST para clasificar radiografÃ­as de tÃ³rax como NORMAL o PNEUMONIA",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configurar CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECCIÃ“N 3: CARGAR MODELO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MODEL_DIR = Path(__file__).parent / ("models")
IMG_SIZE = (224, 224)

# Intentar cargar modelo en orden de preferencia
model = None
model_name = None

model_priority = [
    ('vgg16_finetuned.h5', 'VGG16 Fine-tuned'),
    ('vgg16_best.h5', 'VGG16 Best'),
    ('baseline_best.h5', 'CNN Baseline')
]

for filename, name in model_priority:
    model_path = MODEL_DIR / filename
    if model_path.exists():
        try:
            model = keras.models.load_model(model_path)
            model_name = name
            logger.info(f"âœ… Modelo cargado: {name}")
            break
        except Exception as e:
            logger.error(f"âŒ Error cargando {name}: {e}")
            continue

if model is None:
    logger.error("âŒ No se pudo cargar ningÃºn modelo")
else:
    logger.info(f"âœ… API lista con modelo: {model_name}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECCIÃ“N 4: FUNCIONES AUXILIARES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def preprocess_image(image: Image.Image) -> np.ndarray:
    """
    Preprocesa imagen para el modelo
    
    Args:
        image: Imagen PIL
        
    Returns:
        np.ndarray: Array normalizado (1, 224, 224, 3)
    """
    # Convertir a RGB si es necesario
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # Redimensionar
    image = image.resize(IMG_SIZE)
    
    # Convertir a array y normalizar
    img_array = np.array(image)
    img_array = img_array.astype('float32') / 255.0
    
    # Agregar dimensiÃ³n de batch
    img_array = np.expand_dims(img_array, axis=0)
    
    return img_array

def make_prediction(img_array: np.ndarray) -> dict:
    """
    Realiza predicciÃ³n usando el modelo cargado
    
    Args:
        img_array: Imagen preprocesada
        
    Returns:
        dict: Resultado con predicciÃ³n y probabilidades
    """
    if model is None:
        raise HTTPException(
            status_code=503,
            detail="Modelo no disponible. Verifica carpeta 'models/'"
        )
    
    # Hacer predicciÃ³n
    prediction = model.predict(img_array, verbose=0)[0][0]
    
    # Determinar clase
    predicted_class = "PNEUMONIA" if prediction > 0.5 else "NORMAL"
    confidence = float(prediction if prediction > 0.5 else 1 - prediction)
    
    # Construir respuesta
    result = {
        "prediction": predicted_class,
        "confidence": confidence,
        "probabilities": {
            "NORMAL": float(1 - prediction),
            "PNEUMONIA": float(prediction)
        },
        "model_used": model_name,
        "timestamp": datetime.now().isoformat()
    }
    
    return result

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SECCIÃ“N 5: ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
async def root():
    """Endpoint raÃ­z - InformaciÃ³n general"""
    return {
        "message": "API de DetecciÃ³n de NeumonÃ­a",
        "version": "1.0.0",
        "status": "online",
        "model": model_name if model else "No disponible",
        "endpoints": {
            "/": "InformaciÃ³n general",
            "/health": "Estado del sistema",
            "/predict": "POST - PredicciÃ³n con archivo",
            "/predict_base64": "POST - PredicciÃ³n con base64",
            "/docs": "DocumentaciÃ³n Swagger",
            "/redoc": "DocumentaciÃ³n ReDoc"
        },
        "usage": {
            "example_curl": 'curl -X POST "http://api-url/predict" -F "file=@xray.jpg"',
            "example_python": 'requests.post("http://api-url/predict", files={"file": open("xray.jpg", "rb")})'
        }
    }

@app.get("/health")
async def health_check():
    """Endpoint de health check - Verifica estado"""
    return {
        "status": "healthy" if model is not None else "degraded",
        "model_loaded": model is not None,
        "model_name": model_name if model else "None",
        "timestamp": datetime.now().isoformat(),
        "message": "API funcionando correctamente" if model else "API sin modelo cargado"
    }

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    """
    PredicciÃ³n con archivo de imagen
    
    Args:
        file: Archivo de imagen (JPG, PNG)
        
    Returns:
        JSON con predicciÃ³n y probabilidades
    """
    try:
        # Validar tipo
        if not file.content_type.startswith('image/'):
            raise HTTPException(
                status_code=400,
                detail=f"Archivo debe ser imagen. Recibido: {file.content_type}"
            )
        
        # Leer archivo
        contents = await file.read()
        
        # Validar tamaÃ±o (10 MB mÃ¡x)
        if len(contents) > 10 * 1024 * 1024:
            raise HTTPException(
                status_code=413,
                detail="Imagen muy grande. MÃ¡ximo: 10 MB"
            )
        
        # Abrir imagen
        image = Image.open(io.BytesIO(contents))
        
        logger.info(f"ğŸ“¸ Imagen: {file.filename}, TamaÃ±o: {image.size}")
        
        # Preprocesar
        img_array = preprocess_image(image)
        
        # Predecir
        result = make_prediction(img_array)
        
        logger.info(f"âœ… PredicciÃ³n: {result['prediction']} ({result['confidence']:.2%})")
        
        return JSONResponse(content=result)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error procesando imagen: {str(e)}"
        )

@app.post("/predict_base64")
async def predict_base64(data: dict):
    """
    PredicciÃ³n con imagen en base64
    
    Args:
        data: {"image": "base64_string"}
        
    Returns:
        JSON con predicciÃ³n
    """
    try:
        # Validar campo
        if "image" not in data:
            raise HTTPException(
                status_code=400,
                detail='Campo "image" requerido con string base64'
            )
        
        # Decodificar
        try:
            image_data = base64.b64decode(data["image"])
        except Exception as e:
            raise HTTPException(
                status_code=400,
                detail=f"Error decodificando base64: {str(e)}"
            )
        
        # Abrir imagen
        image = Image.open(io.BytesIO(image_data))
        
        logger.info(f"ğŸ“¸ Imagen base64, TamaÃ±o: {image.size}")
        
        # Preprocesar
        img_array = preprocess_image(image)
        
        # Predecir
        result = make_prediction(img_array)
        
        logger.info(f"âœ… PredicciÃ³n: {result['prediction']} ({result['confidence']:.2%})")
        
        return JSONResponse(content=result)
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error procesando imagen: {str(e)}"
        )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PUNTO DE ENTRADA (Solo para desarrollo local)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  API DE DETECCIÃ“N DE NEUMONÃA - SERVIDOR LOCAL                â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    Servidor: http://localhost:8000
    DocumentaciÃ³n: http://localhost:8000/docs
    
    Presiona Ctrl+C para detener
    """)
    
    uvicorn.run(app, host="0.0.0.0", port=8000)